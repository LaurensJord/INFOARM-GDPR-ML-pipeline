{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "487e59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1a99d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statsmodels version: 0.14.6\n",
      "Numpy version: 2.3.3\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statsmodels as sm\n",
    "\n",
    "from statsmodels.stats.contingency_tables import mcnemar, cochrans_q\n",
    "\n",
    "\n",
    "print(\"Statsmodels version:\", sm.__version__)\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b0cd8",
   "metadata": {},
   "source": [
    "# Classification performance results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e2275",
   "metadata": {},
   "source": [
    "McNemar's test (https://machinelearningmastery.com/mcnemars-test-for-machine-learning/)\n",
    "\n",
    "- Recommended when algorithms that are being compared can only be valuated once, e.g. on one test set, as opposed to repeated evaluations via a resampling technique, such as k-fold cross-validation\n",
    "- For algorithms that can be executed only once, McNemar's test is the only test with acceptable Type I error\n",
    "- For cases where it is expensive or impractical to train multiple copies of models (very large models being trained and evaluated on large datasets)\n",
    "- Marginal homogeneity: Whether two models disagree in the same way (or not). It is not commenting on whether one model is more or less accurate or error prone than another\n",
    "- Null hypothesis: The two models disagree to the same amount (models make errors in much the same proportion, just on different instances of the test set)\n",
    "- Alternative hypothesis: The two models disagree in different ways (models have different relative proportion of errors on the test set)\n",
    "- Does not report on the difference in error between the models, only the relative difference in the proportion of error between the models\n",
    "\n",
    "Effect size (odds ratio) for McNemar: https://www.graphpad.com/guides/prism/latest/statistics/stat_how_to_mcnemars_test.htm\n",
    "\n",
    "Confidence interval for odds ratio: https://www.statology.org/confidence-interval-for-odds-ratio/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81ca39fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       8\n",
      "1       8\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "2475    1\n",
      "2476    7\n",
      "2477    0\n",
      "2478    5\n",
      "2479    5\n",
      "Name: label, Length: 2480, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ground truth labels\n",
    "y_true = pd.read_csv('artifacts/bert/eval_predictions.csv')['label']\n",
    "\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82909d",
   "metadata": {},
   "source": [
    "## BERT vs RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2380b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT predictions\n",
      "0       8\n",
      "1       8\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "2475    1\n",
      "2476    7\n",
      "2477    0\n",
      "2478    5\n",
      "2479    5\n",
      "Name: y_pred, Length: 2480, dtype: int64\n",
      "RoBERTa predictions\n",
      "0       8\n",
      "1       8\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "2475    1\n",
      "2476    7\n",
      "2477    0\n",
      "2478    5\n",
      "2479    5\n",
      "Name: y_pred, Length: 2480, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# bert and roberta predictions\n",
    "y_pred_bert = pd.read_csv('artifacts/bert/eval_predictions.csv')['y_pred']\n",
    "y_pred_roberta = pd.read_csv('artifacts/roberta/eval_predictions.csv')['y_pred']\n",
    "\n",
    "print(\"BERT predictions\")\n",
    "print(y_pred_bert)\n",
    "\n",
    "print(\"RoBERTa predictions\")\n",
    "print(y_pred_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57b09f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RoBERTa Correct</th>\n",
       "      <th>RoBERTa Wrong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BERT Correct</th>\n",
       "      <td>n_11</td>\n",
       "      <td>n_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT Wrong</th>\n",
       "      <td>n_01</td>\n",
       "      <td>n_00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             RoBERTa Correct RoBERTa Wrong\n",
       "BERT Correct            n_11          n_10\n",
       "BERT Wrong              n_01          n_00"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contingency table agreement template\n",
    "contingency_table = pd.DataFrame(\n",
    "    [['n_11', 'n_10'],\n",
    "     ['n_01', 'n_00']],\n",
    "    index=['BERT Correct', 'BERT Wrong'],\n",
    "    columns=['RoBERTa Correct', 'RoBERTa Wrong']\n",
    ")\n",
    "\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a3b05f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1866  151]\n",
      " [  97  366]]\n",
      "pvalue      0.0007640413512558077\n",
      "statistic   11.326612903225806\n"
     ]
    }
   ],
   "source": [
    "n_11 = np.sum((y_pred_bert == y_true) & (y_pred_roberta == y_true))\n",
    "n_10 = np.sum((y_pred_bert == y_true) & (y_pred_roberta != y_true))\n",
    "n_01 = np.sum((y_pred_bert != y_true) & (y_pred_roberta == y_true))\n",
    "n_00 = np.sum((y_pred_bert != y_true) & (y_pred_roberta != y_true))\n",
    "\n",
    "table = np.array([[n_11, n_10],\n",
    "                  [n_01, n_00]])\n",
    "\n",
    "print(table)\n",
    "\n",
    "mcnemar_result = mcnemar(table, exact=False)\n",
    "\n",
    "print(mcnemar_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64f3e136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect size (odds ratio) between BERT and RoBERTa: 1.556701030927835\n"
     ]
    }
   ],
   "source": [
    "bert_roberta_odds_ratio = n_10 / n_01\n",
    "\n",
    "print(\"Effect size (odds ratio) between BERT and RoBERTa:\", bert_roberta_odds_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "891e25c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% CI for odds ratio between BERT and RoBERTa: (1.2062655759068208, 2.0089424319930824)\n"
     ]
    }
   ],
   "source": [
    "SE = math.sqrt(1/n_10 + 1/n_01)\n",
    "lower_ci_bert_roberta = math.exp(math.log(bert_roberta_odds_ratio) - 1.96 * SE)\n",
    "upper_ci_bert_roberta = math.exp(math.log(bert_roberta_odds_ratio) + 1.96 * SE)\n",
    "\n",
    "print(f\"95% CI for odds ratio between BERT and RoBERTa: ({lower_ci_bert_roberta}, {upper_ci_bert_roberta})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b18fe",
   "metadata": {},
   "source": [
    "## BERT vs BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa3b275b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT predictions\n",
      "0       8\n",
      "1       8\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "2475    1\n",
      "2476    7\n",
      "2477    0\n",
      "2478    5\n",
      "2479    5\n",
      "Name: y_pred, Length: 2480, dtype: int64\n",
      "RoBERTa predictions\n",
      "0       2\n",
      "1       0\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "2475    0\n",
      "2476    7\n",
      "2477    0\n",
      "2478    5\n",
      "2479    5\n",
      "Name: y_pred, Length: 2480, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# bert and bilstm predictions\n",
    "y_pred_bert = pd.read_csv('artifacts/bert/eval_predictions.csv')['y_pred']\n",
    "y_pred_bilstm = pd.read_csv('artifacts/bilstm/eval_predictions.csv')['y_pred']\n",
    "\n",
    "print(\"BERT predictions\")\n",
    "print(y_pred_bert)\n",
    "\n",
    "print(\"RoBERTa predictions\")\n",
    "print(y_pred_bilstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a3aee5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BiLSTM Correct</th>\n",
       "      <th>BiLSTM Wrong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BERT Correct</th>\n",
       "      <td>n_11</td>\n",
       "      <td>n_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT Wrong</th>\n",
       "      <td>n_01</td>\n",
       "      <td>n_00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             BiLSTM Correct BiLSTM Wrong\n",
       "BERT Correct           n_11         n_10\n",
       "BERT Wrong             n_01         n_00"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contingency table agreement template\n",
    "contingency_table = pd.DataFrame(\n",
    "    [['n_11', 'n_10'],\n",
    "     ['n_01', 'n_00']],\n",
    "    index=['BERT Correct', 'BERT Wrong'],\n",
    "    columns=['BiLSTM Correct', 'BiLSTM Wrong']\n",
    ")\n",
    "\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74a366a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1507  510]\n",
      " [ 130  333]]\n",
      "pvalue      9.73091609290084e-51\n",
      "statistic   224.4390625\n"
     ]
    }
   ],
   "source": [
    "n_11 = np.sum((y_pred_bert == y_true) & (y_pred_bilstm == y_true))\n",
    "n_10 = np.sum((y_pred_bert == y_true) & (y_pred_bilstm != y_true))\n",
    "n_01 = np.sum((y_pred_bert != y_true) & (y_pred_bilstm == y_true))\n",
    "n_00 = np.sum((y_pred_bert != y_true) & (y_pred_bilstm != y_true))\n",
    "\n",
    "table = np.array([[n_11, n_10],\n",
    "                  [n_01, n_00]])\n",
    "\n",
    "print(table)\n",
    "\n",
    "mcnemar_result = mcnemar(table, exact=False)\n",
    "\n",
    "print(mcnemar_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7246b291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect size (odds ratio) between BERT and BiLSTM: 3.923076923076923\n"
     ]
    }
   ],
   "source": [
    "bert_bilstm_odds_ratio = n_10 / n_01\n",
    "\n",
    "print(\"Effect size (odds ratio) between BERT and BiLSTM:\", bert_bilstm_odds_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00a22381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% CI for odds ratio between BERT and RoBERTa: (3.2358963906250886, 4.756188297303813)\n"
     ]
    }
   ],
   "source": [
    "SE = math.sqrt(1/n_10 + 1/n_01)\n",
    "lower_ci_bert_bilstm = math.exp(math.log(bert_bilstm_odds_ratio) - 1.96 * SE)\n",
    "upper_ci_bert_bilstm = math.exp(math.log(bert_bilstm_odds_ratio) + 1.96 * SE)\n",
    "\n",
    "print(f\"95% CI for odds ratio between BERT and RoBERTa: ({lower_ci_bert_bilstm}, {upper_ci_bert_bilstm})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86650b",
   "metadata": {},
   "source": [
    "## RoBERTa vs BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3bd5181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT predictions\n",
      "0       8\n",
      "1       8\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "2475    1\n",
      "2476    7\n",
      "2477    0\n",
      "2478    5\n",
      "2479    5\n",
      "Name: y_pred, Length: 2480, dtype: int64\n",
      "BiLSTM predictions\n",
      "0       2\n",
      "1       0\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "2475    0\n",
      "2476    7\n",
      "2477    0\n",
      "2478    5\n",
      "2479    5\n",
      "Name: y_pred, Length: 2480, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# roberta and bilstm predictions\n",
    "y_pred_roberta = pd.read_csv('artifacts/roberta/eval_predictions.csv')['y_pred']\n",
    "y_pred_bilstm = pd.read_csv('artifacts/bilstm/eval_predictions.csv')['y_pred']\n",
    "\n",
    "print(\"BERT predictions\")\n",
    "print(y_pred_roberta)\n",
    "\n",
    "print(\"BiLSTM predictions\")\n",
    "print(y_pred_bilstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e03478b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BiLSTM Correct</th>\n",
       "      <th>BiLSTM Wrong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RoBERTa Correct</th>\n",
       "      <td>n_11</td>\n",
       "      <td>n_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RoBERTa Wrong</th>\n",
       "      <td>n_01</td>\n",
       "      <td>n_00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                BiLSTM Correct BiLSTM Wrong\n",
       "RoBERTa Correct           n_11         n_10\n",
       "RoBERTa Wrong             n_01         n_00"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contingency table agreement template\n",
    "contingency_table = pd.DataFrame(\n",
    "    [['n_11', 'n_10'],\n",
    "     ['n_01', 'n_00']],\n",
    "    index=['RoBERTa Correct', 'RoBERTa Wrong'],\n",
    "    columns=['BiLSTM Correct', 'BiLSTM Wrong']\n",
    ")\n",
    "\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa68d3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1470  493]\n",
      " [ 167  350]]\n",
      "pvalue      1.1101260240194846e-36\n",
      "statistic   160.03787878787878\n"
     ]
    }
   ],
   "source": [
    "n_11 = np.sum((y_pred_roberta == y_true) & (y_pred_bilstm == y_true))\n",
    "n_10 = np.sum((y_pred_roberta == y_true) & (y_pred_bilstm != y_true))\n",
    "n_01 = np.sum((y_pred_roberta != y_true) & (y_pred_bilstm == y_true))\n",
    "n_00 = np.sum((y_pred_roberta != y_true) & (y_pred_bilstm != y_true))\n",
    "\n",
    "table = np.array([[n_11, n_10],\n",
    "                  [n_01, n_00]])\n",
    "\n",
    "print(table)\n",
    "\n",
    "mcnemar_result = mcnemar(table, exact=False)\n",
    "\n",
    "print(mcnemar_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba8d1ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect size (odds ratio) between RoBERTa and BiLSTM: 2.9520958083832336\n"
     ]
    }
   ],
   "source": [
    "roberta_bilstm_odds_ratio = n_10 / n_01\n",
    "\n",
    "print(\"Effect size (odds ratio) between RoBERTa and BiLSTM:\", roberta_bilstm_odds_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d7a1770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% CI for odds ratio between BERT and RoBERTa: (2.476949587943646, 3.518387981851867)\n"
     ]
    }
   ],
   "source": [
    "SE = math.sqrt(1/n_10 + 1/n_01)\n",
    "lower_ci_roberta_bilstm = math.exp(math.log(roberta_bilstm_odds_ratio) - 1.96 * SE)\n",
    "upper_ci_roberta_bilstm = math.exp(math.log(roberta_bilstm_odds_ratio) + 1.96 * SE)\n",
    "\n",
    "print(f\"95% CI for odds ratio between BERT and RoBERTa: ({lower_ci_roberta_bilstm}, {upper_ci_roberta_bilstm})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66f053",
   "metadata": {},
   "source": [
    "## Cochran's Q test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7107703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df          2479\n",
      "pvalue      4.80109917232847e-192\n",
      "statistic   5175.821077702536\n"
     ]
    }
   ],
   "source": [
    "# https://stats.stackexchange.com/questions/108047/cochrans-q-mcnemar-tests-together\n",
    "\n",
    "cochrans_q_result = cochrans_q(\n",
    "    np.vstack([y_pred_bert, y_pred_roberta, y_pred_bilstm])\n",
    ")\n",
    "\n",
    "print(cochrans_q_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8829459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.043529866418422\n"
     ]
    }
   ],
   "source": [
    "# https://stats.stackexchange.com/questions/9867/effect-size-of-cochrans-q#:~:text=2%20Answers,%CE%B72Q%20is%20maximized.\n",
    "\n",
    "Q = cochrans_q_result.statistic\n",
    "n = 2480\n",
    "k = 3\n",
    "\n",
    "R = (Q - (k -1)) / ((k - 1) * (n - 1))\n",
    "\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4dafb0",
   "metadata": {},
   "source": [
    "### Homogeneity of covariance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "efdafb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15190032, 0.1087049 , 0.07084201],\n",
       "       [0.1087049 , 0.16507551, 0.07029516],\n",
       "       [0.07084201, 0.07029516, 0.2244647 ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.statisticshowto.com/homogeneity-of-covariance/\n",
    "\n",
    "np.cov(np.vstack([y_pred_bert == y_true, y_pred_roberta == y_true, y_pred_bilstm == y_true]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9981a2c",
   "metadata": {},
   "source": [
    "# Computational performance results for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "345ee9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT CPU cycles:  27805200000.0\n",
      "RoBERTa CPU cycles:  26448000000.0\n",
      "BiLSTM CPU cycles:  11623200000.0\n"
     ]
    }
   ],
   "source": [
    "# inference of computational performance\n",
    "\n",
    "inference_bert_computational_performance = {\"tag\": \"EVAL - BERT - OPP115\", \"wall_time_sec\": 7.4860155309997936, \"cpu_user_sec\": 7.790000000000001, \"cpu_system_sec\": 0.19999999999999973, \"ram_delta_mb\": 302.109375, \"gpu_peak_mem_mb\": 764.2900390625}\n",
    "inference_roberta_computational_performance = {\"tag\": \"EVAL - ROBERTA - OPP115\", \"wall_time_sec\": 7.067067456000586, \"cpu_user_sec\": 7.369999999999999, \"cpu_system_sec\": 0.22999999999999998, \"ram_delta_mb\": 307.03125, \"gpu_peak_mem_mb\": 822.2880859375}\n",
    "inference_bilstm_computational_performance = {\"tag\": \"EVAL - BiLSTM - OPP115\", \"wall_time_sec\": 1.3576289749998978, \"cpu_user_sec\": 2.92, \"cpu_system_sec\": 0.4199999999999997, \"ram_delta_mb\": 429.4140625, \"gpu_peak_mem_mb\": 356.1904296875}\n",
    "\n",
    "inference_bert_cpu_total = inference_bert_computational_performance['cpu_user_sec'] + inference_bert_computational_performance['cpu_system_sec']\n",
    "inference_roberta_cpu_total = inference_roberta_computational_performance['cpu_user_sec'] + inference_roberta_computational_performance['cpu_system_sec']\n",
    "inference_bilstm_cpu_total = inference_bilstm_computational_performance['cpu_user_sec'] + inference_bilstm_computational_performance['cpu_system_sec']\n",
    "\n",
    "inference_cpu_frequency = 3.48 * 10**9 # 3.48 GHz in Hz\n",
    "\n",
    "inference_bert_cpu_cycles = inference_bert_cpu_total * inference_cpu_frequency\n",
    "inference_roberta_cpu_cycles = inference_roberta_cpu_total * inference_cpu_frequency\n",
    "inference_bilstm_cpu_cycles = inference_bilstm_cpu_total * inference_cpu_frequency\n",
    "\n",
    "print(\"BERT CPU cycles: \", inference_bert_cpu_cycles)\n",
    "print(\"RoBERTa CPU cycles: \", inference_roberta_cpu_cycles)\n",
    "print(\"BiLSTM CPU cycles: \", inference_bilstm_cpu_cycles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82622874",
   "metadata": {},
   "source": [
    "# Computational performance results for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d85dd74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT training CPU total seconds:  2149.36\n",
      "RoBERTa training CPU total seconds:  2155.84\n",
      "BiLSTM training CPU total seconds:  17.19\n"
     ]
    }
   ],
   "source": [
    "# traning of computational performance\n",
    "\n",
    "training_bilstm_computational_performance = {\"tag\": \"Train BiLSTM\", \"wall_time_sec\": 12.607763934999639, \"cpu_user_sec\": 14.98, \"cpu_system_sec\": 2.21, \"ram_delta_mb\": 714.87890625, \"gpu_peak_mem_mb\": 659.55126953125}\n",
    "training_bert_computational_performance = {\"tag\": \"TRAINING - BERT - OPP115\", \"wall_time_sec\": 2156.2268697129994, \"cpu_user_sec\": 2144.84, \"cpu_system_sec\": 4.52, \"ram_delta_mb\": 1324.76171875, \"gpu_peak_mem_mb\": 7489.453125}\n",
    "training_roberta_computational_performance = {\"tag\": \"TRAINING - RoBERTa - OPP115\", \"wall_time_sec\": 2161.9818342470007, \"cpu_user_sec\": 2150.05, \"cpu_system_sec\": 5.79, \"ram_delta_mb\": 1342.984375, \"gpu_peak_mem_mb\": 7664.5234375}\n",
    "\n",
    "training_bert_cpu_total = training_bert_computational_performance['cpu_user_sec'] + training_bert_computational_performance['cpu_system_sec']\n",
    "training_roberta_cpu_total = training_roberta_computational_performance['cpu_user_sec'] + training_roberta_computational_performance['cpu_system_sec']\n",
    "training_bilstm_cpu_total = training_bilstm_computational_performance['cpu_user_sec'] + training_bilstm_computational_performance['cpu_system_sec']\n",
    "\n",
    "print(\"BERT training CPU total seconds: \", training_bert_cpu_total)\n",
    "print(\"RoBERTa training CPU total seconds: \", training_roberta_cpu_total)\n",
    "print(\"BiLSTM training CPU total seconds: \", training_bilstm_cpu_total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
