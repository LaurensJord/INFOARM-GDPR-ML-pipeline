{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Le3b8pkJsNF4",
    "outputId": "94cf6692-9379-47ec-e7bd-d7fb01577c38"
   },
   "source": [
    "## Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8Zw41Y0IEaF",
    "outputId": "cf7d69d9-afc4-4e6d-cd39-c9de1fddd87a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from -r requirements.txt (line 1)) (0.12.4)\n",
      "Requirement already satisfied: numpy==1.22.4 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from -r requirements.txt (line 2)) (1.22.4)\n",
      "Requirement already satisfied: pandas==1.5.3 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from -r requirements.txt (line 3)) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from -r requirements.txt (line 4)) (1.7.2)\n",
      "Requirement already satisfied: torch==1.12.1 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from -r requirements.txt (line 5)) (1.12.1)\n",
      "Requirement already satisfied: transformers==4.18.0 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from -r requirements.txt (line 6)) (4.18.0)\n",
      "Requirement already satisfied: sentence-transformers==2.2.2 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from -r requirements.txt (line 7)) (2.2.2)\n",
      "Requirement already satisfied: setfit==0.7.0 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from -r requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: datasets==2.13.0 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from -r requirements.txt (line 9)) (2.13.0)\n",
      "Requirement already satisfied: openpyxl==3.0.9 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from -r requirements.txt (line 10)) (3.0.9)\n",
      "Collecting huggingface_hub==0.10.1\n",
      "  Using cached huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\laurens\\appdata\\roaming\\python\\python310\\site-packages (from pandas==1.5.3->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\laurens\\appdata\\roaming\\python\\python310\\site-packages (from torch==1.12.1->-r requirements.txt (line 5)) (4.15.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from transformers==4.18.0->-r requirements.txt (line 6)) (0.1.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from transformers==4.18.0->-r requirements.txt (line 6)) (6.0.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from transformers==4.18.0->-r requirements.txt (line 6)) (4.67.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from transformers==4.18.0->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from transformers==4.18.0->-r requirements.txt (line 6)) (3.20.0)\n",
      "Requirement already satisfied: requests in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from transformers==4.18.0->-r requirements.txt (line 6)) (2.32.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\laurens\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.18.0->-r requirements.txt (line 6)) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from transformers==4.18.0->-r requirements.txt (line 6)) (2025.11.3)\n",
      "Requirement already satisfied: torchvision in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 7)) (0.13.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 7)) (1.13.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 7)) (3.9.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 7)) (0.2.1)\n",
      "Requirement already satisfied: evaluate>=0.3.0 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from setfit==0.7.0->-r requirements.txt (line 8)) (0.4.6)\n",
      "Requirement already satisfied: xxhash in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from datasets==2.13.0->-r requirements.txt (line 9)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from datasets==2.13.0->-r requirements.txt (line 9)) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from datasets==2.13.0->-r requirements.txt (line 9)) (2025.12.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from datasets==2.13.0->-r requirements.txt (line 9)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\laurens\\appdata\\local\\python\\pythoncore-3.10-64\\lib\\site-packages (from datasets==2.13.0->-r requirements.txt (line 9)) (0.3.6)\n",
      "INFO: pip is looking at multiple versions of setfit to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting setfit==0.7.0\n",
      "  Using cached setfit-0.7.0-py3-none-any.whl (45 kB)\n",
      "INFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sentence-transformers==2.2.2\n",
      "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting transformers==4.18.0\n",
      "  Using cached transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torch==1.12.1\n",
      "  Using cached torch-1.12.1-cp310-cp310-win_amd64.whl (162.2 MB)\n",
      "INFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pandas==1.5.3\n",
      "  Using cached pandas-1.5.3-cp310-cp310-win_amd64.whl (10.4 MB)\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy==1.22.4\n",
      "  Using cached numpy-1.22.4-cp310-cp310-win_amd64.whl (14.7 MB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested huggingface_hub==0.10.1\n",
      "    transformers 4.18.0 depends on huggingface-hub<1.0 and >=0.1.0\n",
      "    sentence-transformers 2.2.2 depends on huggingface-hub>=0.4.0\n",
      "    datasets 2.13.0 depends on huggingface-hub<1.0.0 and >=0.11.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install -r requirements.txt (line 6), -r requirements.txt (line 7), datasets==2.13.0 and huggingface_hub==0.10.1 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y imblearn imbalanced-learn scikit-learn sklearn\n",
    "%pip install \"scikit-learn==1.1.3\" \"imbalanced-learn==0.9.1\"\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RLTGo0soIEhl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laurens\\AppData\\Local\\Python\\pythoncore-3.10-64\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2Nwb0d7XIlNU"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_safe_tags' from 'sklearn.utils._tags' (c:\\Users\\Laurens\\AppData\\Local\\Python\\pythoncore-3.10-64\\lib\\site-packages\\sklearn\\utils\\_tags.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Resources\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mut\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"Load the training dataset\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Input/train_set.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\gitprojects\\UU\\INFOARM_gdpr_ml_pipeline\\INFOARM-GDPR-ML-pipeline\\./Resources\\utils.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, StratifiedKFold\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomOverSampler\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn, optim\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader, WeightedRandomSampler\n",
      "File \u001b[1;32mc:\\Users\\Laurens\\AppData\\Local\\Python\\pythoncore-3.10-64\\lib\\site-packages\\imblearn\\__init__.py:52\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[1;32mc:\\Users\\Laurens\\AppData\\Local\\Python\\pythoncore-3.10-64\\lib\\site-packages\\imblearn\\ensemble\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`imblearn.ensemble` module include methods generating\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03munder-sampled subsets combined inside an ensemble.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bagging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BalancedBaggingClassifier\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_easy_ensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EasyEnsembleClassifier\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_forest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BalancedRandomForestClassifier\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_weight_boosting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RUSBoostClassifier\n",
      "File \u001b[1;32mc:\\Users\\Laurens\\AppData\\Local\\Python\\pythoncore-3.10-64\\lib\\site-packages\\imblearn\\ensemble\\_easy_ensemble.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _partition_estimators\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NotFittedError\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _safe_tags\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_is_fitted\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_safe_tags' from 'sklearn.utils._tags' (c:\\Users\\Laurens\\AppData\\Local\\Python\\pythoncore-3.10-64\\lib\\site-packages\\sklearn\\utils\\_tags.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./Resources\")\n",
    "import utils as ut\n",
    "\n",
    "\"\"\"Load the training dataset\"\"\"\n",
    "df = pd.read_csv(\"./Input/train_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1NG19tc3svt"
   },
   "source": [
    "## Create directories for Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRtvOEcxknkV"
   },
   "outputs": [],
   "source": [
    "parent_directory = \"./\"\n",
    "ut.create_directory('Models', parent_directory)\n",
    "ut.create_directory('Results', parent_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSYh5GlJ4Bk6"
   },
   "source": [
    "## Create sub directories for saving binary and multi-class models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L41Bw3PShz7C"
   },
   "outputs": [],
   "source": [
    "parent_directory = \"./Models\"\n",
    "ut.create_directory('binary', parent_directory)\n",
    "ut.create_directory('multiclass', parent_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMIrFjMs4SRL"
   },
   "source": [
    "## Create sub directories for saving results of the binary and multi-class models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SxtAmi-kz8d"
   },
   "outputs": [],
   "source": [
    "parent_directory = \"./Results\"\n",
    "ut.create_directory('binary', parent_directory)\n",
    "ut.create_directory('multiclass', parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBdVskr-kRAR"
   },
   "outputs": [],
   "source": [
    "model_path = \"./Models/binary/\"\n",
    "results_path = \"./Results/binary/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nk7-JiMP4k4e"
   },
   "source": [
    "## Train all binary models on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6EzTodUlIGA"
   },
   "outputs": [],
   "source": [
    "# ut.start_experiments_binary_original_dataset(df, results_path, model_path, 'exp_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNgLPsNzlIPg"
   },
   "outputs": [],
   "source": [
    "model_path = \"./Models/multiclass/\"\n",
    "results_path = \"./Results/multiclass/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otW1k2Jm4tSz"
   },
   "source": [
    "## Train all multi-class models on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mchmcXHlIWg"
   },
   "outputs": [],
   "source": [
    "ut.start_experiments_MCC_original_dataset(df, results_path, \"exp_1\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
